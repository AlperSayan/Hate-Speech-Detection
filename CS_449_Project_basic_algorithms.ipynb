{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OrFou2Mpcy8",
    "outputId": "cdf9d2e5-f4c8-42a7-aae1-9697808a6101"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used google colab to to train our model therefore i can not change the paths\n",
    "# I would need to download big sized libraries in order to run this notebook\n",
    "# if you would like to test this notebook please configure the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link below is our work on colab and should work without needing the change paths\n",
    "\n",
    "#https://colab.research.google.com/drive/1oEW6oTTxAux3Sw4aORyVaDvhKJhRuC5G?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "0ffKSa_Ipd8l",
    "outputId": "be68230e-db5b-449c-9441-15333ff69b74"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_en = pd.read_csv('/content/drive/MyDrive/hateval2019/hateval2019_en_train.csv')\n",
    "df_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "CzeLY06Apd_5",
    "outputId": "a14e7f21-39e2-4b47-85a4-06e0ae914fc4"
   },
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv('/content/drive/MyDrive/tr_hate_eval.csv')\n",
    "\n",
    "df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzJjoPXdpeCI"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000 # make the top list of words (common words)\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>' # OOV = Out of Vocabulary\n",
    "training_portion = .8\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNA4hAGwqCer"
   },
   "outputs": [],
   "source": [
    "def get_text_and_labels(en=True):\n",
    "  texts = []\n",
    "  labels = []\n",
    "  if en:\n",
    "    with open(\"/content/drive/MyDrive/hateval2019/hateval2019_en_train.csv\", 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        punctuation='@/\\,;.:!?$123456789¿¡'\n",
    "        table=str.maketrans(dict.fromkeys(punctuation))\n",
    "\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            labels.append(row[2])\n",
    "            text = row[1].lower()\n",
    "            text = text.translate(table)\n",
    "            for word in STOPWORDS:\n",
    "                token = ' ' + word + ' '\n",
    "                text = text.replace(token, ' ')\n",
    "                text = text.replace(' ', ' ')\n",
    "            texts.append(text)\n",
    "\n",
    "  else:\n",
    "    with open(\"/content/drive/MyDrive/tr_hate_eval.csv\", 'r') as csvfile:\n",
    "      reader = csv.reader(csvfile, delimiter=',')\n",
    "      punctuation='#@/\\,;.:!?$123456789¿¡'\n",
    "      table=str.maketrans(dict.fromkeys(punctuation))\n",
    "\n",
    "      next(reader)\n",
    "      for row in reader:\n",
    "          labels.append(row[1])\n",
    "          text = row[0].lower()\n",
    "          text = text.translate(table)\n",
    "          for word in STOPWORDS:\n",
    "              token = ' ' + word + ' '\n",
    "              text = text.replace(token, ' ')\n",
    "              text = text.replace(' ', ' ')\n",
    "          texts.append(text)\n",
    "\n",
    "  return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlRAx9iqq4M6"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(texts, labels):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.10, random_state=42)\n",
    "\n",
    "  y_train = np.array(y_train, dtype=np.int)\n",
    "  y_test = np.array(y_test, dtype=np.int)\n",
    "  tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "  tokenizer.fit_on_texts(X_train)\n",
    "  word_index = tokenizer.word_index\n",
    "  train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "  train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "  test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "  test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "  return train_padded, y_train, test_padded, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTIrCBObpeEb"
   },
   "outputs": [],
   "source": [
    "text_en, labels_en = get_text_and_labels(en=True)\n",
    "text_tr, labels_tr = get_text_and_labels(en=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9naeYW_peGX"
   },
   "outputs": [],
   "source": [
    "x_train_en, y_train_en, x_test_en, y_test_en = preprocess_text(text_en, labels_en)\n",
    "x_train_tr, y_train_tr, x_test_tr, y_test_tr = preprocess_text(text_tr, labels_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHPJJtuVqour"
   },
   "outputs": [],
   "source": [
    "# starting with perceptron\n",
    "\n",
    "# listing hyperparameters to be adjusted\n",
    "\n",
    "penalty = ['l2', 'l1', 'elasticnet', None]\n",
    "alpha = [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "fit_intercept = [True, False]\n",
    "tol = [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "perceptrons = []\n",
    "params_all = []\n",
    "\n",
    "for i in range(len(penalty)):\n",
    "    for j in range(len(alpha)):\n",
    "        for k in range(len(fit_intercept)):\n",
    "            for x in range(len(tol)):\n",
    "                pen = penalty[i]\n",
    "                alp = alpha[j]\n",
    "                fit_inter = fit_intercept[k]\n",
    "                tol_ = tol[i]\n",
    "                if pen is not None:\n",
    "                    perceptron = Perceptron(penalty=pen, alpha=alp, fit_intercept=fit_inter, tol=tol_)\n",
    "                else:\n",
    "                    perceptron = Perceptron(fit_intercept=fit_inter, tol=tol_)\n",
    "                \n",
    "                perceptrons.append(perceptron)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr8t49E8s5Jq"
   },
   "outputs": [],
   "source": [
    "# for deleting same objects since i made a mistake in the cell above but because it would be too long i will change them here\n",
    "\n",
    "perceptrons_new = []\n",
    "params_new  = []\n",
    "\n",
    "for i in range(len(perceptrons)):\n",
    "    params_of_the_perceptron = str(perceptrons[i].get_params)\n",
    "    if params_of_the_perceptron not in params_new:\n",
    "        params_new.append(params_of_the_perceptron)\n",
    "        perceptrons_new.append(perceptrons[i])\n",
    "\n",
    "perceptrons = perceptrons_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04lHMJCNs5L9"
   },
   "outputs": [],
   "source": [
    "predictions_perceptron = []\n",
    "accs_perceptron = []\n",
    "max_acc_perceptron = 0\n",
    "max_acc_perceptron_ind = 0\n",
    "\n",
    "\n",
    "for i in range(len(perceptrons)):\n",
    "    perceptrons[i].fit(x_train_en, y_train_en)\n",
    "    prediction = perceptrons[i].predict(x_test_en)\n",
    "    predictions_perceptron.append(prediction)\n",
    "\n",
    "    score = accuracy_score(y_test_en, predictions_perceptron[i])\n",
    "    \n",
    "    if score > max_acc_perceptron:\n",
    "        max_acc_perceptron = score\n",
    "        max_acc_perceptron_ind = i\n",
    "        \n",
    "    accs_perceptron.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJmlBoe-s5OP"
   },
   "outputs": [],
   "source": [
    "# preparing sorted list\n",
    "idx_acc_sorted_perceptron = sorted(range(len(accs_perceptron)),key=accs_perceptron.__getitem__)\n",
    "accs_sorted_perceptron = []\n",
    "perceptrons_sorted = []\n",
    "\n",
    "for i in range(len(accs_perceptron)):\n",
    "    accs_sorted_perceptron.append(accs_perceptron[idx_acc_sorted_perceptron[i]])\n",
    "    perceptrons_sorted.append(perceptrons[idx_acc_sorted_perceptron[i]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6CXjFhQs5Qw"
   },
   "outputs": [],
   "source": [
    "# getting perceptron objects parameters\n",
    "\n",
    "params_all_perceptron = []\n",
    "\n",
    "for i in range(len(perceptrons_sorted)):\n",
    "    params = str(perceptrons_sorted[i].get_params)\n",
    "    params = params[params.find(\"(\")+1:params.find(\")\")]\n",
    "    params_all_perceptron.append(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "Mw8jE6z4s5S5",
    "outputId": "042f02e9-82e7-442c-8813-d3d9ccab1557"
   },
   "outputs": [],
   "source": [
    "# plotting highest accuracy 10 perceptron objects since printing all the objects needs to big plot \n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(np.arange(10), accs_sorted_perceptron[-10:])\n",
    "plt.xticks(np.arange(10), params_all_perceptron[-10:], rotation= 90)\n",
    "plt.xlabel('hyperparameters')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Perceptron accuracies with adjusted hyperparameters plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf3th8a6s5YV"
   },
   "outputs": [],
   "source": [
    "# the algorithm fails to converge on reasonable times\n",
    "\n",
    "# svcs = []\n",
    "\n",
    "# c_s = [0.9, 1, 1.1]\n",
    "# kernels = ['linear', 'poly', 'rbf'] \n",
    "# degrees = [3]\n",
    "# gammas = ['scale']\n",
    "\n",
    "# for i in range(len(c_s)):\n",
    "#     for j in range(len(kernels)):\n",
    "#         c = c_s[i]\n",
    "#         kernel = kernels[j]\n",
    "        \n",
    "#         if kernel == 'poly':\n",
    "#             for k in range(len(degrees)):\n",
    "#                 degree = degrees[k]\n",
    "#                 for x in range(len(gammas)):\n",
    "#                     gamma = gammas[x]\n",
    "#                     svc_ = SVC(C=c, kernel=kernel, degree=degree, gamma=gamma)\n",
    "#                     svcs.append(svc_)\n",
    "                    \n",
    "#         elif kernel == 'rbf' or kernel == 'sigmoid':\n",
    "#             for k in range(len(gammas)):\n",
    "#                 gamma = gammas[k]\n",
    "#                 scv_ = SVC(C=c, kernel=kernel, gamma=gamma)\n",
    "#                 svcs.append(svc_)\n",
    "\n",
    "                \n",
    "#         else:\n",
    "#             svc_ = SVC(C=c, kernel=kernel)\n",
    "#             svcs.append(svc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJEYKSy7vwLC"
   },
   "outputs": [],
   "source": [
    "# predictions_svm = []\n",
    "# accs_svm = []\n",
    "# max_acc_svm = 0\n",
    "# max_acc_svm_ind = 0\n",
    "\n",
    "# for i in range(len(svcs)):\n",
    "#     svcs[i].fit(x_train_en, y_train_en)\n",
    "#     print('train i done', i)\n",
    "#     prediction = svcs[i].predict(x_test_en)\n",
    "#     print('test i done', i)\n",
    "#     predictions_svm.append(prediction)\n",
    "\n",
    "#     score = accuracy_score(y_test_en, predictions_svm[i])\n",
    "    \n",
    "#     if score > max_acc_svm:\n",
    "#         max_acc_svm = score\n",
    "#         max_acc_svm_ind = i\n",
    "        \n",
    "#     accs_svm.append(score)\n",
    "\n",
    "# idx_acc_sorted_svm = sorted(range(len(accs_svm)),key=accs_svm.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbmWWk46vwNc"
   },
   "outputs": [],
   "source": [
    "# # preparing sorted list\n",
    "\n",
    "# accs_sorted_svm = []\n",
    "# svm_sorted = []\n",
    "\n",
    "# for i in range(len(accs_svm)):\n",
    "#     accs_sorted_svm.append(accs_svm[idx_acc_sorted_svm[i]])\n",
    "#     svm_sorted.append(svcs[idx_acc_sorted_svm[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KA9to16mvwPZ"
   },
   "outputs": [],
   "source": [
    "# # getting svm objects parameters\n",
    "\n",
    "# params_all_svm = []\n",
    "\n",
    "# for i in range(len(svm_sorted)):\n",
    "#     params = str(svm_sorted[i].get_params)\n",
    "#     params = params[params.find(\"(\")+1:params.find(\")\")]\n",
    "#     params_all_svm.append(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mObFc8wpvwRq"
   },
   "outputs": [],
   "source": [
    "# # plotting svm objects with ordered accuracies\n",
    "\n",
    "# plt.figure(figsize=(20, 5))\n",
    "# plt.plot(np.arange(len(accs_sorted_svm)), accs_sorted_svm)\n",
    "# plt.xticks(np.arange(len(params_all_svm)), params_all_svm, rotation= 90)\n",
    "# plt.xlabel('hyperparameters')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.title('SVM accuracies with adjusted hyperparameters plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FHvDPSVvwTz"
   },
   "outputs": [],
   "source": [
    "criterions = ['entropy'] # initially tested with gini as well did not give desired performance\n",
    "splitters = ['random'] # random works better\n",
    "max_depths = [10, 15, None]\n",
    "min_samples_splits = [1.0, 2, 3]\n",
    "min_samples_leafs = [1, 2, 3]\n",
    "max_features = ['sqrt', 'log2'] # max features none does not yield the optimal results therefore i dropped it\n",
    "\n",
    "decision_trees = []\n",
    "\n",
    "for i in range(len(criterions)):\n",
    "    for j in range(len(splitters)):\n",
    "        for k in range(len(max_depths)):\n",
    "            for l in range(len(min_samples_splits)):\n",
    "                for x in range(len(min_samples_leafs)):\n",
    "                    for y in range(len(max_features)):\n",
    "                    \n",
    "                        criterion = criterions[i]\n",
    "                        splitter = splitters[j]\n",
    "                        max_depth = max_depths[k]\n",
    "                        min_samples_split = min_samples_splits[l]\n",
    "                        min_samples_leaf = min_samples_leafs[x]\n",
    "                        max_feature = max_features[y]\n",
    "                        \n",
    "                        decision_tree = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_features=max_feature)\n",
    "                        decision_trees.append(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjb28wRkvwW_"
   },
   "outputs": [],
   "source": [
    "predictions_decision_tree = []\n",
    "accs_decision_tree = []\n",
    "max_acc_decision_tree = 0\n",
    "max_acc_decision_tree_ind = 0\n",
    "\n",
    "for i in range(len(decision_trees)):\n",
    "    decision_trees[i].fit(x_train_en, y_train_en)\n",
    "    prediction = decision_trees[i].predict(x_test_en)\n",
    "    predictions_decision_tree.append(prediction)\n",
    "\n",
    "    if score > max_acc_decision_tree:\n",
    "        max_acc_decision_tree = score\n",
    "        max_acc_decision_tree_ind = i\n",
    "        \n",
    "    accs_decision_tree.append(score)\n",
    "\n",
    "idx_acc_sorted_decision_tree = sorted(range(len(accs_decision_tree)),key=accs_decision_tree.__getitem__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vucRGBX_vwZL"
   },
   "outputs": [],
   "source": [
    "# preparing sorted list\n",
    "\n",
    "accs_sorted_decision_tree = []\n",
    "decision_tree_sorted = []\n",
    "\n",
    "for i in range(len(decision_trees)):\n",
    "    accs_sorted_decision_tree.append(accs_decision_tree[idx_acc_sorted_decision_tree[i]])\n",
    "    decision_tree_sorted.append(decision_trees[idx_acc_sorted_decision_tree[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pheSP4HgwokP"
   },
   "outputs": [],
   "source": [
    "# getting decision tree objects parameters\n",
    "\n",
    "params_all_decision_tree = []\n",
    "\n",
    "for i in range(len(decision_tree_sorted)):\n",
    "    params = str(decision_tree_sorted[i].get_params)\n",
    "    params = params[params.find(\"(\")+1:params.find(\")\")]\n",
    "    params_all_decision_tree.append(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "eO4jM_arwomp",
    "outputId": "bc45c62b-06f6-4d00-9e0b-b35316fdacdc"
   },
   "outputs": [],
   "source": [
    "# plotting highest 10 accuracy decision tree objects \n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(np.arange(10), accs_sorted_decision_tree[-10:])\n",
    "plt.xticks(np.arange(10), params_all_decision_tree[-10:], rotation= 90)\n",
    "plt.xlabel('hyperparameters')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Decision tree accuracies with adjusted hyperparameters plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfQyf7Xkwooo"
   },
   "outputs": [],
   "source": [
    "n_estimators_ = [50, 100]\n",
    "criterions = ['entropy'] # I initially test with gini but entropy worked better for timing constraints I dropped it\n",
    "max_depths = [10, None]\n",
    "min_samples_splits = [1.0, 2, 3]\n",
    "min_samples_leafs = [1, 2, 3]\n",
    "max_features_ = ['sqrt', 'log2'] # Initially tested with none did not give good results\n",
    "\n",
    "random_forest_classifiers = []\n",
    "\n",
    "for i in range(len(n_estimators_)):\n",
    "    for j in range(len(criterions)):\n",
    "        for k in range(len(max_depths)):\n",
    "            for l in range(len(min_samples_splits)):\n",
    "                for x in range(len(min_samples_leafs)):\n",
    "                    for y in range(len(max_features_)):\n",
    "                        n_estimators = n_estimators_[i]\n",
    "                        criterion = criterions[j]\n",
    "                        max_depth = max_depths[k]\n",
    "                        min_samples_split = min_samples_splits[l]\n",
    "                        min_samples_leaf = min_samples_leafs[x]\n",
    "                        max_features = max_features_[y]\n",
    "                        random_forest_classfier = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_features=max_features)\n",
    "                        random_forest_classifiers.append(random_forest_classfier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gpkPxYZwovJ"
   },
   "outputs": [],
   "source": [
    "# fitting random forest objects\n",
    "predictions_random_forest = []\n",
    "accs_random_forest = []\n",
    "max_acc_random_forest = 0\n",
    "max_acc_random_forest_ind = 0\n",
    "\n",
    "\n",
    "for i in range(len(random_forest_classifiers)):\n",
    "    random_forest_classifiers[i].fit(x_train_en, y_train_en)\n",
    "    prediction = random_forest_classifiers[i].predict(x_test_en)\n",
    "    predictions_random_forest.append(prediction)\n",
    "\n",
    "    score = accuracy_score(y_test_en, predictions_random_forest[i])\n",
    "    \n",
    "    if score > max_acc_random_forest:\n",
    "        max_acc_random_forest = score\n",
    "        max_acc_random_forest_ind = i\n",
    "        \n",
    "    accs_random_forest.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2j-hs1nBx6HC"
   },
   "outputs": [],
   "source": [
    "idx_acc_sorted_random_forest = sorted(range(len(accs_random_forest)),key=accs_random_forest.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXLMWYcJx6JV"
   },
   "outputs": [],
   "source": [
    "# preparing sorted list\n",
    "\n",
    "accs_sorted_random_forest = []\n",
    "random_forest_sorted = []\n",
    "\n",
    "for i in range(len(accs_random_forest)):\n",
    "    accs_sorted_random_forest.append(accs_random_forest[idx_acc_sorted_random_forest[i]])\n",
    "    random_forest_sorted.append(random_forest_classifiers[idx_acc_sorted_random_forest[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXNiVTzvx6LX"
   },
   "outputs": [],
   "source": [
    "# getting random forest objects parameters\n",
    "\n",
    "params_all_random_forest = []\n",
    "\n",
    "for i in range(len(random_forest_sorted)):\n",
    "    params = str(random_forest_sorted[i].get_params)\n",
    "    params = params[params.find(\"(\")+1:params.find(\")\")]\n",
    "    params_all_random_forest.append(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "id": "GBtlIbCax6Nf",
    "outputId": "c18858af-6b67-4ba3-bfbe-ac960e975b1a"
   },
   "outputs": [],
   "source": [
    "# plotting every four random forest objects to see the effect of hyperparameters\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(np.arange(10), accs_sorted_random_forest[-10:])\n",
    "plt.xticks(np.arange(10), params_all_random_forest[-10:], rotation= 90)\n",
    "plt.xlabel('hyperparameters')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Random forest accuracies with adjusted hyperparameters plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDpdYOm2x6R9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IE7slqKx6Ud"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS-449-Project-basic-algorithms.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
